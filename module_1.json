{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Feature Engineering Examples\n",
       "This notebook showcases various Python code snippets for feature engineering, including numeric transformations, categorical encoding, text feature extraction, scaling, and a simple time-series example."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Numeric Feature Extraction & Transformation"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "import numpy as np\n",
       "import pandas as pd\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "\n",
       "# Example dataset\n",
       "data = {\n",
       "    'price': [10000, 20000, 30000, 40000],\n",
       "    'mileage': [50000, 30000, 20000, 10000]\n",
       "}\n",
       "df = pd.DataFrame(data)\n",
       "print(\"Original Data:\")\n",
       "print(df)\n",
       "\n",
       "# Log transform for skewed data\n",
       "df['price_log'] = np.log1p(df['price'])\n",
       "df['mileage_log'] = np.log1p(df['mileage'])\n",
       "\n",
       "# Standardization (z-score normalization)\n",
       "scaler = StandardScaler()\n",
       "scaled_features = scaler.fit_transform(df[['price', 'mileage']])\n",
       "df[['price_scaled', 'mileage_scaled']] = scaled_features\n",
       "\n",
       "print(\"\\nTransformed Data:\")\n",
       "print(df)"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Categorical Feature Encoding"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "import pandas as pd\n",
       "from sklearn.preprocessing import LabelEncoder\n",
       "from category_encoders import TargetEncoder\n",
       "\n",
       "df = pd.DataFrame({\n",
       "    'color': ['red', 'blue', 'green', 'blue', 'red'],\n",
       "    'transmission': ['manual', 'auto', 'manual', 'auto', 'manual'],\n",
       "    'target': [0, 1, 0, 1, 0]  # Target variable\n",
       "})\n",
       "\n",
       "print(\"Original Categorical Data:\")\n",
       "print(df)\n",
       "\n",
       "# 1. One-hot encoding\n",
       "df_onehot = pd.get_dummies(df, columns=['color', 'transmission'])\n",
       "\n",
       "# 2. Label encoding (for ordinal categories)\n",
       "le = LabelEncoder()\n",
       "df['color_label'] = le.fit_transform(df['color'])\n",
       "\n",
       "# 3. Target encoding\n",
       "te = TargetEncoder()\n",
       "df['color_target_encoded'] = te.fit_transform(df[['color']], df['target'])\n",
       "\n",
       "print(\"\\nOne-Hot Encoded Data:\")\n",
       "print(df_onehot)\n",
       "print(\"\\nLabel Encoded Data:\")\n",
       "print(df[['color', 'color_label']])\n",
       "print(\"\\nTarget Encoded Data:\")\n",
       "print(df[['color', 'color_target_encoded']])"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Text Feature Extraction (Bag-of-Words & TF-IDF)"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "texts = [\n",
       "    \"Machine learning is awesome and powerful\",\n",
       "    \"Python is great for machine learning projects\",\n",
       "    \"I enjoy exploring new python libraries for NLP\"\n",
       "]\n",
       "\n",
       "# 1. Simple bag-of-words with better visualization\n",
       "count_vec = CountVectorizer()\n",
       "bow_matrix = count_vec.fit_transform(texts)\n",
       "feature_names = count_vec.get_feature_names_out()\n",
       "\n",
       "# Create a DataFrame for better visualization of bag-of-words\n",
       "bow_df = pd.DataFrame(\n",
       "    bow_matrix.toarray(),\n",
       "    columns=feature_names,\n",
       "    index=['Text 1', 'Text 2', 'Text 3']\n",
       ")\n",
       "print(\"Bag-of-Words Counts (each cell shows word frequency):\")\n",
       "print(bow_df)\n",
       "\n",
       "# 2. TF-IDF vectorization with better visualization\n",
       "tfidf = TfidfVectorizer()\n",
       "tfidf_matrix = tfidf.fit_transform(texts)\n",
       "tfidf_feature_names = tfidf.get_feature_names_out()\n",
       "\n",
       "tfidf_df = pd.DataFrame(\n",
       "    tfidf_matrix.toarray(),\n",
       "    columns=tfidf_feature_names,\n",
       "    index=['Text 1', 'Text 2', 'Text 3']\n",
       ")\n",
       "print(\"\\nTF-IDF Scores (higher values = more important words in document):\")\n",
       "print(tfidf_df.round(3))  # Round to 3 decimal places for readability\n",
       "\n",
       "# 3. Add explanation of how TF-IDF is calculated for a specific word\n",
       "def explain_tfidf(word, corpus, vectorizer):\n",
       "    # Get the index of the word\n",
       "    word_idx = np.where(vectorizer.get_feature_names_out() == word)[0][0]\n",
       "    \n",
       "    # Get document frequencies\n",
       "    df_count = sum(1 for doc in corpus if word in doc.lower().split())\n",
       "    idf = np.log((1 + len(corpus)) / (1 + df_count)) + 1  # The scikit-learn formula\n",
       "    \n",
       "    print(f\"\\nExplanation of TF-IDF calculation for word '{word}':\")\n",
       "    print(f\"Document frequency (df): {df_count} out of {len(corpus)} documents\")\n",
       "    print(f\"Inverse document frequency (idf): log({1 + len(corpus)}/{1 + df_count}) + 1 = {idf:.3f}\")\n",
       "    \n",
       "    for i, doc in enumerate(corpus):\n",
       "        words = doc.lower().split()\n",
       "        tf = words.count(word.lower()) / len(words)\n",
       "        tfidf_val = tf * idf\n",
       "        print(f\"\\nText {i+1}:\")\n",
       "        print(f\"  Term frequency (tf): {words.count(word.lower())}/{len(words)} = {tf:.3f}\")\n",
       "        print(f\"  TF-IDF score: {tf:.3f} Ã— {idf:.3f} = {tfidf_val:.3f}\")\n",
       "\n",
       "# Let's explain a word that appears in multiple documents\n",
       "explain_tfidf(\"learning\", texts, tfidf)"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Feature Scaling & Normalization"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "import pandas as pd\n",
       "import numpy as np\n",
       "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
       "from sklearn.preprocessing import Normalizer, PowerTransformer, QuantileTransformer\n",
       "\n",
       "np.random.seed(42)\n",
       "data = {\n",
       "    'feature1': np.random.exponential(scale=2.0, size=1000),\n",
       "    'feature2': np.random.normal(loc=5, scale=2, size=1000),\n",
       "    'feature3': np.random.lognormal(mean=0, sigma=0.5, size=1000)\n",
       "}\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "# Example: Min-Max Scaling\n",
       "minmax = MinMaxScaler()\n",
       "df_minmax = pd.DataFrame(minmax.fit_transform(df), columns=df.columns)\n",
       "\n",
       "# Example: Standardization\n",
       "standard = StandardScaler()\n",
       "df_standard = pd.DataFrame(standard.fit_transform(df), columns=df.columns)\n",
       "\n",
       "# Compare original vs. transformed distributions\n",
       "print(\"Original Skewness:\", df.skew())"
      ],
      "execution_count": null,
      "outputs": []
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Time-Series Feature Extraction Example"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "import pandas as pd\n",
       "import numpy as np\n",
       "\n",
       "# Generate a simple time-series dataset\n",
       "dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n",
       "values = [100, 105, 102, 98, 110, 108, 107, 115, 116, 118]\n",
       "df_ts = pd.DataFrame({'date': dates, 'value': values})\n",
       "\n",
       "# Set the date column as the index for time-series operations\n",
       "df_ts.set_index('date', inplace=True)\n",
       "\n",
       "# Calculate rolling mean and rolling std over a window of 3 days\n",
       "df_ts['rolling_mean_3'] = df_ts['value'].rolling(window=3).mean()\n",
       "df_ts['rolling_std_3'] = df_ts['value'].rolling(window=3).std()\n",
       "\n",
       "print(\"Time-Series Data with Rolling Features:\")\n",
       "print(df_ts)"
      ],
      "execution_count": null,
      "outputs": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }
   