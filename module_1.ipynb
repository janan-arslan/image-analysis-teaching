{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15efc81f",
   "metadata": {},
   "source": [
    "# Numeric Feature Extraction & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas numpy matplotlib category_encoders"
   ]
  }
 ],
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367159d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "  'price': [10000, 20000, 30000, 40000],\n",
    "  'mileage': [50000, 30000, 20000, 10000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "\n",
    "# Log transform for skewed data\n",
    "df['price_log'] = np.log1p(df['price'])\n",
    "df['mileage_log'] = np.log1p(df['mileage'])\n",
    "\n",
    "# Standardization (z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[['price', 'mileage']])\n",
    "df[['price_scaled', 'mileage_scaled']] = scaled_features\n",
    "\n",
    "print(\"\\nTransformed Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2061f67",
   "metadata": {},
   "source": [
    "# Categorical Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a26c78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "df = pd.DataFrame({\n",
    "  'color': ['red', 'blue', 'green', 'blue', 'red'],\n",
    "  'transmission': ['manual', 'auto', 'manual', 'auto', 'manual'],\n",
    "  'target': [0, 1, 0, 1, 0]  # Target variable\n",
    "})\n",
    "\n",
    "print(\"Original Categorical Data:\")\n",
    "print(df)\n",
    "\n",
    "# 1. One-hot encoding\n",
    "df_onehot = pd.get_dummies(df, columns=['color', 'transmission'])\n",
    "\n",
    "# 2. Label encoding (for ordinal categories)\n",
    "le = LabelEncoder()\n",
    "df['color_label'] = le.fit_transform(df['color'])\n",
    "\n",
    "# 3. Target encoding\n",
    "te = TargetEncoder()\n",
    "df['color_target_encoded'] = te.fit_transform(df[['color']], df['target'])\n",
    "\n",
    "print(\"\\nOne-Hot Encoded Data:\")\n",
    "print(df_onehot)\n",
    "print(\"\\nLabel Encoded Data:\")\n",
    "print(df[['color', 'color_label']])\n",
    "print(\"\\nTarget Encoded Data:\")\n",
    "print(df[['color', 'color_target_encoded']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b6859",
   "metadata": {},
   "source": [
    "# Text Feature Extraction (Bag-of-Words & TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c567a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "texts = [\n",
    "    \"Machine learning is awesome and powerful\",\n",
    "    \"Python is great for machine learning projects\",\n",
    "    \"I enjoy exploring new python libraries for NLP\"\n",
    "]\n",
    "\n",
    "# 1. Simple bag-of-words with better visualization\n",
    "count_vec = CountVectorizer()\n",
    "bow_matrix = count_vec.fit_transform(texts)\n",
    "feature_names = count_vec.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization of bag-of-words\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=['Text 1', 'Text 2', 'Text 3']\n",
    ")\n",
    "print(\"Bag-of-Words Counts (each cell shows word frequency):\")\n",
    "print(bow_df)\n",
    "\n",
    "# 2. TF-IDF vectorization with better visualization\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(texts)\n",
    "tfidf_feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization of TF-IDF\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_feature_names,\n",
    "    index=['Text 1', 'Text 2', 'Text 3']\n",
    ")\n",
    "print(\"\\nTF-IDF Scores (higher values = more important words in document):\")\n",
    "print(tfidf_df.round(3))  # Round to 3 decimal places for readability\n",
    "\n",
    "# 3. Add explanation of how TF-IDF is calculated for a specific word\n",
    "def explain_tfidf(word, corpus, vectorizer):\n",
    "    # Get the index of the word\n",
    "    word_idx = np.where(vectorizer.get_feature_names_out() == word)[0][0]\n",
    "    \n",
    "    # Get document frequencies\n",
    "    df = sum(1 for doc in corpus if word in doc.lower().split())\n",
    "    idf = np.log((1 + len(corpus)) / (1 + df)) + 1  # The scikit-learn formula\n",
    "    \n",
    "    print(f\"\\nExplanation of TF-IDF calculation for word '{word}':\")\n",
    "    print(f\"Document frequency (df): {df} out of {len(corpus)} documents\")\n",
    "    print(f\"Inverse document frequency (idf): log({1 + len(corpus)}/{1 + df}) + 1 = {idf:.3f}\")\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        words = doc.lower().split()\n",
    "        tf = words.count(word.lower()) / len(words)\n",
    "        tfidf = tf * idf\n",
    "        print(f\"\\nText {i+1}:\")\n",
    "        print(f\"  Term frequency (tf): {words.count(word.lower())}/{len(words)} = {tf:.3f}\")\n",
    "        print(f\"  TF-IDF score: {tf:.3f} Ã— {idf:.3f} = {tfidf:.3f}\")\n",
    "\n",
    "# Let's explain a word that appears in multiple documents\n",
    "explain_tfidf(\"learning\", texts, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef5763",
   "metadata": {},
   "source": [
    "# Feature Scaling & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7998f26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import Normalizer, PowerTransformer, QuantileTransformer\n",
    "\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'feature1': np.random.exponential(scale=2.0, size=1000),\n",
    "    'feature2': np.random.normal(loc=5, scale=2, size=1000),\n",
    "    'feature3': np.random.lognormal(mean=0, sigma=0.5, size=1000)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Example: Min-Max Scaling\n",
    "minmax = MinMaxScaler()\n",
    "df_minmax = pd.DataFrame(minmax.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Example: Standardization\n",
    "standard = StandardScaler()\n",
    "df_standard = pd.DataFrame(standard.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Compare original vs. transformed distributions\n",
    "print(\"Original Skewness:\", df.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b65dac",
   "metadata": {},
   "source": [
    "# Time-Series Feature Extraction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822fb0cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate a simple time-series dataset\n",
    "dates = pd.date_range(\"2023-01-01\", periods=10, freq=\"D\")\n",
    "values = [100, 105, 102, 98, 110, 108, 107, 115, 116, 118]\n",
    "df = pd.DataFrame({'date': dates, 'value': values})\n",
    "\n",
    "# Set the date column as the index for time-series operations\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Calculate rolling mean and rolling std over a window of 3 days\n",
    "df['rolling_mean_3'] = df['value'].rolling(window=3).mean()\n",
    "df['rolling_std_3'] = df['value'].rolling(window=3).std()\n",
    "\n",
    "print(\"Time-Series Data with Rolling Features:\")\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
